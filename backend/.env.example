# ================================
# EduRAG Backend Environment Variables
# ================================

# --------------------------------
# Server Configuration
# --------------------------------
PORT=5001
NODE_ENV=development

# --------------------------------
# Database Configuration
# --------------------------------

# MongoDB Connection URI
# Local: mongodb://localhost:27017/edurag
# Atlas: mongodb+srv://<username>:<password>@cluster.mongodb.net/edurag
MONGODB_URI=mongodb://localhost:27017/edurag

# ChromaDB Server URL
# Default local: http://localhost:8000
# Docker: http://chromadb:8000
CHROMA_URL=http://localhost:8000

# --------------------------------
# AI API Keys
# --------------------------------

# Groq API Key (for fast LLM - Layer 1 routing)
# Get from: https://console.groq.com/keys
# Format: gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
GROQ_API_KEY=your_groq_api_key_here

# Google Gemini API Key (for deep understanding - Layer 3)
# Get from: https://aistudio.google.com/app/apikey
# Format: AIzaSyxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# Note: This is labeled as GEMMA_API_KEY in code for legacy reasons
GEMMA_API_KEY=your_gemini_api_key_here

# HuggingFace API Key (for text embeddings)
# Get from: https://huggingface.co/settings/tokens
# Format: hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
HUGGINGFACE_API_KEY=your_huggingface_api_key_here

# --------------------------------
# Important Notes
# --------------------------------
# 1. NEVER commit .env file with real API keys
# 2. Create a copy as .env and add your actual keys
# 3. Keep API keys secure and rotate them regularly
# 4. Use different keys for development and production

# ================================
# OFFLINE MODE CONFIGURATION
# ================================

# Set to 'offline' to use Ollama (no internet required)
# Set to 'online' to use Groq/Gemini APIs
# Set to 'auto' to automatically detect and prefer offline if available
USE_OFFLINE_MODE=offline

# --------------------------------
# Ollama Configuration (for offline mode)
# --------------------------------

# Ollama server URL (default: http://localhost:11434)
OLLAMA_URL=http://localhost:11434

# Chat/Reasoning model (DeepSeek R1)
OLLAMA_CHAT_MODEL=deepseek-r1:1.5b

# Embedding model
OLLAMA_EMBED_MODEL=embeddinggemma:latest

# --------------------------------
# Ollama Setup Instructions
# --------------------------------
# 1. Install Ollama from https://ollama.ai/download
# 2. Start Ollama: `ollama serve`
# 3. Pull required models:
#    - ollama pull deepseek-r1:1.5b
#    - ollama pull embeddinggemma:latest (or your custom model)
# 4. Set USE_OFFLINE_MODE=offline in .env

# ================================
# FILE STORAGE CONFIGURATION
# ================================

# Directory for storing uploaded files (for preview functionality)
# In Docker: mount this as a volume for persistence
# Default: ./uploads/files (relative to backend root)
FILE_STORAGE_DIR=./uploads/files

# For Docker/containerized deployment, use absolute path:
# FILE_STORAGE_DIR=/app/uploads/files
# And mount a volume: -v /host/path/to/uploads:/app/uploads/files
